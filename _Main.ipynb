{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except:\n",
    "    import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "import h5py\n",
    "import math\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from deepst.models.STResNet import stresnet\n",
    "from deepst.config import Config\n",
    "import deepst.metrics as metrics\n",
    "\n",
    "import deepst.datasets.TaxiBJ as taxi\n",
    "import deepst.datasets.BikeNYC as bike\n",
    "\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras import backend as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.set_image_data_format('channels_first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "HYPER-PARAMETER FOR EXPERIMENTS' CONFIGURATIONS\n",
    "\"\"\"\n",
    "\n",
    "### ASSIGN THE DATASET HERE\n",
    "DATASET = 'taxi'\n",
    "\n",
    "\"\"\"\n",
    "Assertion to ensure dataset correctness\n",
    "\"\"\"\n",
    "data_options = ['taxi', 'bike']\n",
    "assert DATASET in data_options, \"Dataset not found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Assigning global variables\n",
    "\"\"\"\n",
    "meta_data=True\n",
    "### Dataset configuration (Fixed by the authors)\n",
    "if DATASET == \"taxi\":\n",
    "    LOAD_DATA = taxi.load_data\n",
    "    map_height, map_width = 32, 32  # grid size\n",
    "    T = 48 # 30 mins\n",
    "    meteorol_data=True\n",
    "    holiday_data=True\n",
    "    lr = 0.0002  # learning rate\n",
    "    len_closeness = 3  # length of closeness dependent sequence\n",
    "    len_period = 1  # length of peroid dependent sequence\n",
    "    len_trend = 1  # length of trend dependent sequence\n",
    "    days_test = 7 * 4 # Last 4 weeks\n",
    "    m_factor = 1.0\n",
    "    _patience = 2\n",
    "    EPHOCS = 500  # number of epoch at training stage\n",
    "    EPHOCS_CONT = 100  # number of epoch at training (cont) stage\n",
    "elif DATASET == 'bike':\n",
    "    LOAD_DATA = bike.load_data\n",
    "    map_height, map_width = 16, 8  # grid size\n",
    "    T = 24 # 1 hour\n",
    "    meteorol_data=False\n",
    "    holiday_data=False\n",
    "    lr = 0.0002  # learning rate\n",
    "    len_closeness = 3  # length of closeness dependent sequence\n",
    "    len_period = 4  # length of peroid dependent sequence\n",
    "    len_trend = 4  # length of trend dependent sequence\n",
    "    days_test = 10 # Last 10 days\n",
    "    # For NYC Bike data, there are 81 available grid-based areas, each of\n",
    "    # which includes at least ONE bike station. Therefore, we modify the final\n",
    "    # RMSE by multiplying the following factor (i.e., factor).\n",
    "    nb_area = 81\n",
    "    m_factor = math.sqrt(1. * map_height * map_width / nb_area)\n",
    "    _patience = 5\n",
    "    EPHOCS = 500  # number of epoch at training stage\n",
    "    EPHOCS_CONT = 100  # number of epoch at training (cont) stage\n",
    "    \n",
    "print(m_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXED PARAMETERS\n",
    "DATAPATH = Config().DATAPATH  # data path, you may set your own data path with the global envirmental variable DATAPATH\n",
    "CACHEDATA = True  # cache data or NOT\n",
    "path_cache = os.path.join(DATAPATH, 'CACHE')  # cache path\n",
    "batch_size = 32  # batch size\n",
    "\n",
    "nb_flow = 2  # there are two types of flows: inflow and outflow\n",
    "# divide data into two subsets: Train & Test\n",
    "len_test = T * days_test\n",
    "path_result = 'RET'\n",
    "path_model = 'MODEL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ENSURE DIRECTORIES EXIST\n",
    "\"\"\"\n",
    "if os.path.isdir(path_result) is False:\n",
    "    os.mkdir(path_result)\n",
    "if os.path.isdir(path_model) is False:\n",
    "    os.mkdir(path_model)\n",
    "if CACHEDATA and os.path.isdir(path_cache) is False:\n",
    "    os.mkdir(path_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Memory management for Keras (tensorflow) so it does not occupy the whole GPU memory\n",
    "Reference: \n",
    "- https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth                               (For Tensorflow 2.0)\n",
    "- https://kobkrit.com/using-allow-growth-memory-option-in-tensorflow-and-keras-dc8c8081bc96     (For Tensorflow 1.0)\n",
    "\"\"\"\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\" # Using the GPU with ID = 0\n",
    "# ### For debugging purpose\n",
    "# from tensorflow.python.client import device_lib\n",
    "# print(device_lib.list_local_devices())\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU') # Restrict TensorFlow to only use the first GPU\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)    # Dynamic memory allocation to save memory space\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "    except RuntimeError as e:\n",
    "        # Visible devices must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(external_dim, nb_residual_unit=2, bn=False, fusion=True):\n",
    "    c_conf = (len_closeness, nb_flow, map_height,\n",
    "              map_width) if len_closeness > 0 else None\n",
    "    p_conf = (len_period, nb_flow, map_height,\n",
    "              map_width) if len_period > 0 else None\n",
    "    t_conf = (len_trend, nb_flow, map_height,\n",
    "              map_width) if len_trend > 0 else None\n",
    "\n",
    "    model = stresnet(c_conf=c_conf, p_conf=p_conf, t_conf=t_conf,\n",
    "                     external_dim=external_dim, nb_residual_unit=nb_residual_unit, bn=bn, fusion=fusion)\n",
    "    adam = Adam(lr=lr)\n",
    "    model.compile(loss='mse', optimizer=adam, metrics=[metrics.rmse])\n",
    "    model.summary()\n",
    "    # from keras.utils.visualize_util import plot\n",
    "    # plot(model, to_file='model.png', show_shapes=True)\n",
    "    return model\n",
    "\n",
    "\n",
    "def read_cache(fname, pre='preprocessing.pkl'):\n",
    "    mmn = pickle.load(open(pre, 'rb'))\n",
    "\n",
    "    f = h5py.File(fname, 'r')\n",
    "    num = int(f['num'][()])\n",
    "    X_train, Y_train, X_test, Y_test = [], [], [], []\n",
    "    for i in range(num):\n",
    "        X_train.append(f['X_train_%i' % i][()])\n",
    "        X_test.append(f['X_test_%i' % i][()])\n",
    "    Y_train = f['Y_train'][()]\n",
    "    Y_test = f['Y_test'][()]\n",
    "    external_dim = int(f['external_dim'][()])\n",
    "    timestamp_train = f['T_train'][()]\n",
    "    timestamp_test = f['T_test'][()]\n",
    "    f.close()\n",
    "\n",
    "    return X_train, Y_train, X_test, Y_test, mmn, external_dim, timestamp_train, timestamp_test\n",
    "\n",
    "\n",
    "def cache(fname, X_train, Y_train, X_test, Y_test, external_dim, timestamp_train, timestamp_test):\n",
    "    h5 = h5py.File(fname, 'w')\n",
    "    h5.create_dataset('num', data=len(X_train))\n",
    "\n",
    "    for i, data in enumerate(X_train):\n",
    "        h5.create_dataset('X_train_%i' % i, data=data)\n",
    "    # for i, data in enumerate(Y_train):\n",
    "    for i, data in enumerate(X_test):\n",
    "        h5.create_dataset('X_test_%i' % i, data=data)\n",
    "    h5.create_dataset('Y_train', data=Y_train)\n",
    "    h5.create_dataset('Y_test', data=Y_test)\n",
    "    external_dim = -1 if external_dim is None else int(external_dim)\n",
    "    h5.create_dataset('external_dim', data=external_dim)\n",
    "    h5.create_dataset('T_train', data=timestamp_train)\n",
    "    h5.create_dataset('T_test', data=timestamp_test)\n",
    "    h5.close()\n",
    "\n",
    "def main(nb_residual_unit=2, bn=False, fusion=True, epochs=EPHOCS, nb_epoch_cont=EPHOCS_CONT):\n",
    "    # load data\n",
    "    print(\"loading data...\")\n",
    "    ts = time.time()\n",
    "    fname = os.path.join(DATAPATH, 'CACHE', '{}_C{}_P{}_T{}.h5'.format(\n",
    "        DATASET, len_closeness, len_period, len_trend))\n",
    "    pre = 'preprocessing_%s.pkl' % DATASET\n",
    "    if os.path.exists(fname) and CACHEDATA:\n",
    "        X_train, Y_train, X_test, Y_test, mmn, external_dim, timestamp_train, timestamp_test = read_cache(\n",
    "            fname, pre)\n",
    "        print(\"load %s successfully\" % fname)\n",
    "    else:\n",
    "        X_train, Y_train, X_test, Y_test, mmn, external_dim, timestamp_train, timestamp_test = LOAD_DATA(\n",
    "            T=T, nb_flow=nb_flow, len_closeness=len_closeness, len_period=len_period, len_trend=len_trend, len_test=len_test,\n",
    "            preprocess_name=pre, \n",
    "            meta_data=meta_data, meteorol_data=meteorol_data, holiday_data=holiday_data)\n",
    "        if CACHEDATA:\n",
    "            cache(fname, X_train, Y_train, X_test, Y_test,\n",
    "                  external_dim, timestamp_train, timestamp_test)\n",
    "\n",
    "    print(\"\\n days (test): \", [v[:8] for v in timestamp_test[0::T]])\n",
    "    print(\"\\nelapsed time (loading data): %.3f seconds\\n\" % (time.time() - ts))\n",
    "\n",
    "    print('=' * 10)\n",
    "    print(\"compiling model...\")\n",
    "\n",
    "    ts = time.time()\n",
    "    model = build_model(external_dim, nb_residual_unit=nb_residual_unit, bn=bn, fusion=fusion)\n",
    "    hyperparams_name = '{}_c{}.p{}.t{}.resunit{}.lr{}'.format(\n",
    "        DATASET, len_closeness, len_period, len_trend, nb_residual_unit, lr)\n",
    "    fname_param = os.path.join('MODEL', '{}.best.h5'.format(hyperparams_name))\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_rmse', patience=_patience, mode='min')\n",
    "    model_checkpoint = ModelCheckpoint(\n",
    "        fname_param, monitor='val_rmse', verbose=0, save_best_only=True, mode='min')\n",
    "\n",
    "    print(\"\\nelapsed time (compiling model): %.3f seconds\\n\" %\n",
    "          (time.time() - ts))\n",
    "\n",
    "    print('=' * 10)\n",
    "    print(\"training model...\")\n",
    "    ts = time.time()\n",
    "    history = model.fit(X_train, Y_train,\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        validation_split=0.1,\n",
    "                        callbacks=[early_stopping, model_checkpoint],\n",
    "                        verbose=1)\n",
    "    model.save_weights(os.path.join(\n",
    "        'MODEL', '{}.h5'.format(hyperparams_name)), overwrite=True)\n",
    "    pickle.dump((history.history), open(os.path.join(\n",
    "        path_result, '{}.history.pkl'.format(hyperparams_name)), 'wb'))\n",
    "    print(\"\\nelapsed time (training): %.3f seconds\\n\" % (time.time() - ts))\n",
    "\n",
    "    print('=' * 10)\n",
    "    print('evaluating using the model that has the best loss on the valid set')\n",
    "    ts = time.time()\n",
    "    model.load_weights(fname_param)\n",
    "    score = model.evaluate(X_train, Y_train, batch_size=Y_train.shape[\n",
    "                           0] // 48, verbose=0)\n",
    "    print('Train score: %.6f rmse (norm): %.6f rmse (real): %.6f' %\n",
    "          (score[0], score[1], score[1] * (mmn._max - mmn._min) / 2. * m_factor))\n",
    "    score = model.evaluate(\n",
    "        X_test, Y_test, batch_size=Y_test.shape[0], verbose=0)\n",
    "    print('Test score: %.6f rmse (norm): %.6f rmse (real): %.6f' %\n",
    "          (score[0], score[1], score[1] * (mmn._max - mmn._min) / 2. * m_factor))\n",
    "    print(\"\\nelapsed time (eval): %.3f seconds\\n\" % (time.time() - ts))\n",
    "\n",
    "    print('=' * 10)\n",
    "    print(\"training model (cont)...\")\n",
    "    ts = time.time()\n",
    "    fname_param = os.path.join(\n",
    "        'MODEL', '{}.cont.best.h5'.format(hyperparams_name))\n",
    "    model_checkpoint = ModelCheckpoint(\n",
    "        fname_param, monitor='rmse', verbose=0, save_best_only=True, mode='min')\n",
    "    history = model.fit(X_train, Y_train, epochs=nb_epoch_cont, verbose=1, batch_size=batch_size, callbacks=[\n",
    "                        model_checkpoint])\n",
    "    pickle.dump((history.history), open(os.path.join(\n",
    "        path_result, '{}.cont.history.pkl'.format(hyperparams_name)), 'wb'))\n",
    "    model.save_weights(os.path.join(\n",
    "        'MODEL', '{}_cont.h5'.format(hyperparams_name)), overwrite=True)\n",
    "    print(\"\\nelapsed time (training cont): %.3f seconds\\n\" % (time.time() - ts))\n",
    "\n",
    "    print('=' * 10)\n",
    "    print('evaluating using the final model')\n",
    "    score = model.evaluate(X_train, Y_train, batch_size=Y_train.shape[\n",
    "                           0] // 48, verbose=0)\n",
    "    print('Train score: %.6f rmse (norm): %.6f rmse (real): %.6f' %\n",
    "          (score[0], score[1], score[1] * (mmn._max - mmn._min) / 2. * m_factor))\n",
    "    ts = time.time()\n",
    "    score = model.evaluate(\n",
    "        X_test, Y_test, batch_size=Y_test.shape[0], verbose=0)\n",
    "    print('Test score: %.6f rmse (norm): %.6f rmse (real): %.6f' %\n",
    "          (score[0], score[1], score[1] * (mmn._max - mmn._min) / 2. * m_factor))\n",
    "    print(\"\\nelapsed time (eval cont): %.3f seconds\\n\" % (time.time() - ts))\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    return y_pred, Y_test, mmn, m_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE(y_true, y_pred):\n",
    "    assert len(y_true) == len(y_pred), \"Length of y_true must be equal to y_pred\"\n",
    "    rmse = 0.0\n",
    "    for i in range(len(y_true)):\n",
    "        x = (np.sqrt(np.mean((y_true-y_pred)**2)))\n",
    "        rmse += x\n",
    "    rmse /= len(y_true)\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%notify\n",
    "%%timeit -n 1 -r 1\n",
    "\n",
    "y_pred, Y_test, mmn, m_factor = main(nb_residual_unit=2, bn=False)\n",
    "K.clear_session()\n",
    "rmse = RMSE(Y_test, y_pred)\n",
    "print(rmse)\n",
    "RMSE(Y_test, y_pred) * (mmn._max - mmn._min) / 2. * m_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%notify\n",
    "%%timeit -n 1 -r 1\n",
    "### Model L2\n",
    "main(nb_residual_unit=2, bn=False)\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%notify\n",
    "%%timeit -n 1 -r 1\n",
    "### Model L4\n",
    "main(nb_residual_unit=4, bn=False)\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%notify\n",
    "%%timeit -n 1 -r 1\n",
    "### Model L12\n",
    "main(nb_residual_unit=12, bn=False)\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%notify\n",
    "%%timeit -n 1 -r 1\n",
    "### Model L12-BN\n",
    "main(nb_residual_unit=12, bn=True)\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%notify\n",
    "%%timeit -n 1 -r 1\n",
    "### Model L2-BN\n",
    "main(nb_residual_unit=2, bn=True)\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%notify\n",
    "%%timeit -n 1 -r 1\n",
    "### Model L4-BN\n",
    "main(nb_residual_unit=4, bn=True)\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
